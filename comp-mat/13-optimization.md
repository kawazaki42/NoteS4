## Тёмы

1) Оптимизация
   - поиск минимума 1 и неск. переменных
     - поовинное деление
     - метод наискорейшего спуска
2) Интерполяция
   - множеством
   - сплайном
3) Производная
4) Интрегалы
5) Дифференциальные уравнения

---

1) Поиск точки минимума унимодальной непрерывной ф-ции y = f(x),
   определенной на R
   - Унимодальная === единственная точка экстремума
   - а) Локализация точки min === поиск отрезка,
        в котором лежит точка min.
   - б) Уменьшить отрезок, чтобы его длина стала < 2 \epsilon

```
b - a < 2 epsilon; \rlArr (b-a)/2 < epsilon
```

x* - истинная точка min

x~ - приближенное значение точки min

```
| x* - x~ | <= (b-a)/2 < epsilon
```

---

## а) Метод Свенна.

x0 - d; x0; x0 + d

надо в них посчитать значения ф-ции

### Случай I

Если:
- f(x0 - d) > f(x0)
- f(x0 + d) > f(x0)
- (оба) => [a; b] = [x0-d; x0+d]

### Случай II

- f(x0-d) > f(x0)
- f(x0-d) < f(x0)

это значит надо идти вправо

x3 + 8d

пока не получится впандинка как в прошлом случае

Последовательность:

x_n = x_(n-1) + d * 2^(n-1)

Когда остановиться: найдётся номер k:

- f(x_(k-1)) > F(x_k)
- f(x_(k+1)) > F(x_k)
- } => [a, b] = [x_(k-1); x_(k+1)]

### Случай III

- f(x0-d) < f(x0)
- f(x0-d) > f(x0)

Последовательность:

x_n = x_(n-1) - d * 2^(n-1)

Когда остановиться: найдётся номер k:

- f(x_(k-1)) > F(x_k)
- f(x_(k+1)) > F(x_k)
- } => [a, b] = [x_(k+1); x_(k-1)]

(то же самое условие)


## б) Уточнение точки min

 - a = x_0
 - x_1
 - x_2
 - x_3
 - b = x_4

Формулы:

- x_1 = (3a + b) / 4
- x_2 = (2a + 2b) / 4 = (a+b)/2
- x_3 = (a + 3b) / 4

### (I)

Если:

- f(x_1) > f(x2)
- f(x_3) > f(x_2)
- } =>

```pascal
a := x1;
b := x3;
```

### (II)

- f(x_1) < f(x2)
- f(x_3) > f(x_2)
- } =>

```pascal
b := x2;
```

### (III)

- f(x_1) > f(x2)
- f(x_3) < f(x_2)
- } =>

```pascal
a := x2;
```

### Когда остановиться?

(b-a)/2 < epsilon

---

# Многомерная оптимизация

Найти точку минимума min (глобального) ф-ции

y = f(x_1, x_2, ..., x_n)

, непрерывной на R^n

при n=2 рисунок...

оси x_1, x_2, y

там чашка

точка минимума:

x* = (x_1 ^*; x_2 ^*)

Метод наискорейшего спуска:

grad f(X) = {df/dx_1; df/dx_2; df/dx_n}

- показывает направление наискорейшего возрастания ф-ции f(X)

\-grad f(X) - показывает направление наискорейшего убывания f(X)

вектор _АНТИградиента_

---

вид чаши сверху

кидаем точку x_0

-grad f(X) = W(X)

X_1 = argmin_(t >= 0) f(X_0 + t * W(X_0))

X_2 = argmin_(t >= 0) f(X_1 + t * W(X_0))

> NOTE
> W каждый раз разный!

Последовательность X_0, X_1, X_2, ....

Когда остановиться? Несколько условий:

1) Если для X_k: | W(X_k) | < delta - основное условие
2) Для X_k, X_(k-1): | X_k - X_(k-1) | < epsilon

Плохие случаи:

т.н. застревание (маленькими шагами)

## Пример

f(x_1, x_2) = 1/2 (x_1 - 1)^2 + 1/4 (x_2 - 2)^4

Видно что X^* = (1; 2)

X_0 = (0; 0);

grad f(X) = {1/2 * 2 * (x_1 - 1); 1/4 * 4 * (x_2 - 2)^3} = { x_1 - 1 ; 1/4 * 4 * (x_2 - 2)^3}

W(X) = { x_1 - 1 ; 1/4 * 4 * (x_2 - 2)^3}

W(X_0) = {1; 8}

f(X_0 + t * W(X_0)) = f( (0; 0) + t * (1; 8)) = f(t, 8t) = 1/2 (x_1 - 1)^2 + 1/4 (x_2 - 2)^4

Ищем минимум

f' = t-1 + (8t - 2)^3 = 0

t ~= 0.35784

X_1 = (0; 0) + 0.35784 * (1; 8) = (0.35784, 2.86272)
